{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/wbqvbi6o6ip0vz55ua5gp17g4f1k7ve9.png\" width = 300, align = \"center\"></a>\n# <center>Hierarchical Clustering</center>", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <b>Welcome to Lab of Hierarchical Clustering with Python using Scipy package.</b>", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Some Notebook Commands Reminders:\n<ul>\n    <li>Run a cell: CTRL + ENTER</li>\n    <li>Create a cell above a cell: a</li>\n    <li>Create a cell below a cell: b</li>\n    <li>Change a cell to Markdown: m</li>\n    \n    <li>Change a cell to code: y</li>\n</ul>\n\n<b> If you are interested in more keyboard shortcuts, go to Help -> Keyboard Shortcuts </b>", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "#  Hierarchical Clustering - Agglomerative\n\nWe will be looking at a clustering technique, which is <b>Agglomerative Hierarchical Clustering</b>. Remember that agglomerative is the bottom up approach. <br> <br>\nIn this lab, we will be looking at Agglomerative clustering, which is more popular than Divisive clustering. <br> <br>\nWe will also be using Complete Linkage as the Linkage Criteria. <br>\n<b> <i> NOTE: You can also try using Average Linkage wherever Complete Linkage would be used to see the difference! </i> </b>", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "import numpy as np \nfrom scipy import ndimage \nfrom scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix \nfrom matplotlib import pyplot as plt \nfrom sklearn import manifold, datasets \nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.datasets.samples_generator import make_blobs \n%matplotlib inline", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "---\n### Generating Random Data\nWe will be generating a set of data using the <b>make_blobs</b> class. <br> <br>\nInput these parameters into make_blobs:\n<ul>\n    <li> <b>n_samples</b>: The total number of points equally divided among clusters. </li>\n    <ul> <li> Choose a number from 10-1500 </li> </ul>\n    <li> <b>centers</b>: The number of centers to generate, or the fixed center locations. </li>\n    <ul> <li> Choose arrays of x,y coordinates for generating the centers. Have 1-10 centers (ex. centers=[[1,1], [2,5]]) </li> </ul>\n    <li> <b>cluster_std</b>: The standard deviation of the clusters. The larger the number, the further apart the clusters</li>\n    <ul> <li> Choose a number between 0.5-1.5 </li> </ul>\n</ul> <br>\nSave the result to <b>X1</b> and <b>y1</b>.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "X1, y1 = make_blobs(n_samples=50, centers=[[4,4], [-2, -1], [1, 1], [10,4]], cluster_std=0.9)", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Plot the scatter plot of the randomly generated data", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "plt.scatter(X1[:, 0], X1[:, 1], marker='o') ", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "---\n### Agglomerative Clustering\nWe will start by clustering the random data points we just created.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "The <b> AgglomerativeClustering </b> class will require two inputs:\n<ul>\n    <li> <b>n_clusters</b>: The number of clusters to form as well as the number of centroids to generate. </li>\n    <ul> <li> Value will be: 4 </li> </ul>\n    <li> <b>linkage</b>: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. </li>\n    <ul> \n        <li> Value will be: 'complete' </li> \n        <li> <b>Note</b>: It is recommended you try everything with 'average' as well </li>\n    </ul>\n</ul> <br>\nSave the result to a variable called <b> agglom </b>", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "agglom = AgglomerativeClustering(n_clusters = 4, linkage = 'average')", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Fit the model with <b> X2 </b> and <b> y2 </b> from the generated data above.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "agglom.fit(X1,y1)", 
            "execution_count": null, 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Run the following code to show the clustering! <br>\nRemember to read the code and comments to gain more understanding on how the plotting works.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Create a figure of size 6 inches by 4 inches.\nplt.figure(figsize=(6,4))\n\n# These two lines of code are used to scale the data points down,\n# Or else the data points will be scattered very far apart.\n\n# Create a minimum and maximum range of X1.\nx_min, x_max = np.min(X1, axis=0), np.max(X1, axis=0)\n\n# Get the average distance for X1.\nX1 = (X1 - x_min) / (x_max - x_min)\n\n# This loop displays all of the datapoints.\nfor i in range(X1.shape[0]):\n    # Replace the data points with their respective cluster value \n    # (ex. 0) and is color coded with a colormap (plt.cm.spectral)\n    plt.text(X1[i, 0], X1[i, 1], str(y1[i]),\n             color=plt.cm.spectral(agglom.labels_[i] / 10.),\n             fontdict={'weight': 'bold', 'size': 9})\n    \n# Remove the x ticks, y ticks, x and y axis\nplt.xticks([])\nplt.yticks([])\n#plt.axis('off')\n\n\n\n# Display the plot of the original data before clustering\nplt.scatter(X1[:, 0], X1[:, 1], marker='.')\n# Display the plot\nplt.show()", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "---\n### Dendrogram Associated for the Agglomerative Hierarchical Clustering\nRemember that a <b>distance matrix</b> contains the <b> distance from each point to every other point of a dataset </b>. <br>\nUse the function <b> distance_matrix, </b> which requires <b>two inputs</b>. Use the Feature Matrix, <b> X2 </b> as both inputs and save the distance matrix to a variable called <b> dist_matrix </b> <br> <br>\nRemember that the distance values are symmetric, with a diagonal of 0's. This is one way of making sure your matrix is correct. <br> (print out dist_matrix to make sure it's correct)", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "dist_matrix = distance_matrix(X1,X1) \nprint(dist_matrix)", 
            "execution_count": null, 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Using the <b> linkage </b> class from hierarchy, pass in the parameters:\n<ul>\n    <li> The distance matrix </li>\n    <li> 'complete' for complete linkage </li>\n</ul> <br>\nSave the result to a variable called <b> Z </b>", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "Z = hierarchy.linkage(dist_matrix, 'complete')", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Next, we will save the dendrogram to a variable called <b>dendro</b>. In doing this, the dendrogram will also be displayed.\nUsing the <b> dendrogram </b> class from hierarchy, pass in the parameter:\n<ul> <li> Z </li> </ul>", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "dendro = hierarchy.dendrogram(Z)", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Want to learn more?\n\nIBM SPSS Modeler is a comprehensive analytics platform that has many machine learning algorithms. It has been designed to bring predictive intelligence to decisions made by individuals, by groups, by systems \u2013 by your enterprise as a whole. A free trial is available through this course, available here: [SPSS Modeler for Mac users](https://cocl.us/ML0101EN_SPSSMod_mac) and [SPSS Modeler for Windows users](https://cocl.us/ML0101EN_SPSSMod_win)\n\nAlso, you can use Data Science Experience to run these notebooks faster with bigger datasets. Data Science Experience is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, DSX enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of DSX users today with a free account at [Data Science Experience](https://cocl.us/ML0101EN_DSX)", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "---\n# Additional Resources\n<br>\nGeneral Clustering: http://scikit-learn.org/stable/modules/clustering.html \n<br><br>\nHierarchical Clustering: <br>\nhttps://www.youtube.com/watch?v=OcoE7JlbXvY <br>\nhttps://www.youtube.com/watch?v=MIWVfCcHzM4 <br>\nhttps://www.youtube.com/watch?v=2z5wwyv0Zk4", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<hr>\nCopyright &copy; 2016 [Big Data University](https://bigdatauniversity.com/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\u200b", 
            "metadata": {}
        }
    ], 
    "nbformat_minor": 1, 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 1.6", 
            "name": "python2"
        }, 
        "widgets": {
            "state": {}, 
            "version": "1.1.2"
        }, 
        "language_info": {
            "pygments_lexer": "ipython2", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "name": "python", 
            "file_extension": ".py", 
            "version": "2.7.11", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }
}